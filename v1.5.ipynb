{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8227140e4853d37e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TechInAsia Scraper: A Computational Narrative\n",
    "\n",
    "This notebook presents a step-by-step breakdown of a web scraping script designed to extract articles from the TechInAsia website, focusing on the artificial intelligence category. The script utilizes Selenium for dynamic page loading and BeautifulSoup for HTML parsing. The scraped data is then organized into a pandas DataFrame and saved as CSV files.\n",
    "\n",
    "This notebook is structured into five main parts:\n",
    "\n",
    "1.  **Setup and Configuration**: This section covers the necessary imports, logging setup, and the definition of data structures and configuration classes.\n",
    "2.  **Web Driver and Scrolling**: Here, we initialize the Selenium WebDriver and implement a scrolling mechanism to load more content on the page.\n",
    "3.  **Article Parsing**: This part focuses on extracting relevant information from each article element on the page.\n",
    "4.  **Scraping Logic**: This section details the main scraping process, including retry mechanisms and data processing.\n",
    "5.  **Execution and Summary**: Finally, we execute the scraper, display a sample of the results, and provide a summary of the scraping process."
   ],
   "id": "30e17e4aef221ead"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:45.146998Z",
     "start_time": "2025-01-15T03:30:45.131576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code Cell\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from random import uniform\n",
    "import os\n",
    "import re\n",
    "from dateutil import parser\n",
    "import argparse\n",
    "import undetected_chromedriver as uc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "log_dir = \"logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "log_filename = os.path.join(log_dir, f\"logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Article:\n",
    "    \"\"\"Data class for storing article information\"\"\"\n",
    "    article_id: Optional[str]\n",
    "    title: str\n",
    "    article_url: str\n",
    "    source: Optional[str]\n",
    "    source_url: Optional[str]\n",
    "    image_url: Optional[str]\n",
    "    posted_time: Optional[str]\n",
    "    relative_time: str\n",
    "    categories: List[str] = field(default_factory=list)\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    scraped_at: str = field(default_factory=lambda: datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    posted_time_iso: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Convert posted_time to ISO format\"\"\"\n",
    "        if self.posted_time:\n",
    "            try:\n",
    "                dt = parser.parse(self.posted_time)\n",
    "                self.posted_time_iso = dt.isoformat()\n",
    "            except (ValueError, TypeError) as e:\n",
    "                logger.warning(f\"Failed to parse posted_time: {self.posted_time}, error: {e}\")\n",
    "                self.posted_time_iso = None\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert the Article instance to a dictionary\"\"\"\n",
    "        return {\n",
    "            'article_id': self.article_id,\n",
    "            'title': self.title,\n",
    "            'article_url': self.article_url,\n",
    "            'source': self.source,\n",
    "            'source_url': self.source_url,\n",
    "            'image_url': self.image_url,\n",
    "            'posted_time': self.posted_time,\n",
    "            'posted_time_iso': self.posted_time_iso,\n",
    "            'relative_time': self.relative_time,\n",
    "            'categories': ','.join(self.categories),\n",
    "            'tags': ','.join(self.tags),\n",
    "            'scraped_at': self.scraped_at\n",
    "        }\n",
    "\n",
    "class ScraperConfig:\n",
    "    \"\"\"Configuration management for the scraper\"\"\"\n",
    "    DEFAULT_CONFIG = {\n",
    "        'num_articles': 50,\n",
    "        'max_scrolls': 10,\n",
    "        'timeout': 20,\n",
    "        'retry_count': 3,\n",
    "        'scroll_pause_time': 1.5,\n",
    "        'batch_size': 100,\n",
    "        'base_url': 'https://www.techinasia.com/news',\n",
    "        'output_dir': 'output',\n",
    "        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'min_delay': 1,\n",
    "        'max_delay': 3\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize the configuration with default and custom values\"\"\"\n",
    "        self.config = {**self.DEFAULT_CONFIG, **kwargs}\n",
    "        self.__dict__.update(self.config)\n",
    "\n",
    "class ScrollManager:\n",
    "    \"\"\"Manages page scrolling\"\"\"\n",
    "    def __init__(self, driver, config):\n",
    "        self.driver = driver\n",
    "        self.config = config\n",
    "\n",
    "    def scroll_page(self) -> bool:\n",
    "        \"\"\"Scroll the page and return True if new content was loaded\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(uniform(self.config.min_delay, self.config.max_delay))\n",
    "        new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        return new_height > last_height"
   ],
   "id": "2a4dd4c24da8bd27",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:46.102240Z",
     "start_time": "2025-01-15T03:30:46.099569Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7b84920799b6bfc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Web Driver and Scrolling\n",
    "\n",
    "This section focuses on setting up the Selenium WebDriver and managing page scrolling:\n",
    "\n",
    "-   **WebDriver Initialization**: The `setup_driver` method initializes the Chrome WebDriver with specific options, including headless mode and user-agent settings. It also uses `undetected_chromedriver` to avoid detection by anti-bot systems.\n",
    "-   **Scroll Management**: The `ScrollManager` class is responsible for scrolling the page to load more content. The `scroll_page` method scrolls to the bottom of the page and checks if new content has been loaded."
   ],
   "id": "5e9d4ffaffb48ebc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:47.196600Z",
     "start_time": "2025-01-15T03:30:47.190202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code Cell\n",
    "class TechInAsiaScraper:\n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"Initialize the scraper with configuration\"\"\"\n",
    "        self.config = ScraperConfig(**(config or {}))\n",
    "        self.driver = None\n",
    "        self.scroll_manager = None\n",
    "        self._setup_output_directory()\n",
    "        self.processed_article_ids = set()\n",
    "        self.incomplete_articles = 0\n",
    "        self.total_articles = 0\n",
    "\n",
    "    def _setup_output_directory(self):\n",
    "        \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(self.config.output_dir):\n",
    "            os.makedirs(self.config.output_dir)\n",
    "\n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver with retry mechanism\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument(f'user-agent={self.config.user_agent}')\n",
    "\n",
    "        try:\n",
    "            driver_manager = ChromeDriverManager()\n",
    "            driver_path = driver_manager.install()\n",
    "            self.driver = uc.Chrome(\n",
    "                service=Service(driver_path),\n",
    "                options=options,\n",
    "                use_subprocess=True\n",
    "            )\n",
    "            self.scroll_manager = ScrollManager(self.driver, self.config)\n",
    "            logger.info(\"WebDriver initialized successfully üöÄ\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize WebDriver: {e} üòû\")\n",
    "            raise"
   ],
   "id": "5aaeeda95fde25b5",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:47.576803Z",
     "start_time": "2025-01-15T03:30:47.574115Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "72f603260c87854a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:48.229197Z",
     "start_time": "2025-01-15T03:30:48.226648Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "1ee82b2deb3c44df",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Article Parsing\n",
    "\n",
    "This section details how individual article elements are parsed to extract relevant information:\n",
    "\n",
    "-   **Data Extraction**: The `parse_article` method uses BeautifulSoup to find and extract the title, article URL, source, source URL, image URL, posted time, categories, and tags from each article element.\n",
    "-   **Data Cleaning**: The `_clean_article_data` method normalizes the extracted data, replacing 'N/A' values with `None`.\n",
    "-   **Validation**: The `_is_valid_article` method checks if the article has all the required fields, specifically the article URL."
   ],
   "id": "323ed737c60944e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:50.439056Z",
     "start_time": "2025-01-15T03:30:50.426530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code Cell\n",
    "def _is_valid_article(self, article: Article) -> bool:\n",
    "    \"\"\"Validate if the article has all required fields\"\"\"\n",
    "    if not article.article_url:\n",
    "        logger.warning(f\"‚ö†Ô∏è Skipping article with missing article_url: {article.article_id}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _clean_article_data(self, article: Article) -> Article:\n",
    "    \"\"\"Clean and normalize article data\"\"\"\n",
    "    if article.source == 'N/A':\n",
    "        article.source = None\n",
    "    if article.image_url == 'N/A':\n",
    "        article.image_url = None\n",
    "    if article.source_url == 'N/A':\n",
    "        article.source_url = None\n",
    "    return article\n",
    "\n",
    "def parse_article(self, article_element) -> Optional[Article]:\n",
    "    \"\"\"Parse a single article element\"\"\"\n",
    "    article_id = None\n",
    "    title = 'N/A'\n",
    "    article_url = None\n",
    "    source = None\n",
    "    source_url = None\n",
    "    image_url = None\n",
    "    posted_time = None\n",
    "    relative_time = 'N/A'\n",
    "    categories = []\n",
    "    tags = []\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Parsing article: {article_id}\")\n",
    "        content_div = article_element.find('div', class_='post-content')\n",
    "\n",
    "        # Extract article information\n",
    "        logger.info(f\"  - Extracting title...\")\n",
    "        title_element = content_div.find('h3', class_='post-title')\n",
    "        title = title_element.text.strip() if title_element else 'N/A'\n",
    "        logger.info(f\"  - Title extracted: {title}\")\n",
    "\n",
    "        # Get article URL and ID\n",
    "        logger.info(f\"  - Extracting article URL and ID...\")\n",
    "        article_links = [a for a in content_div.find_all('a') if not 'post-source' in a.get('class', [])]\n",
    "        if article_links:\n",
    "            href = article_links[0]['href']\n",
    "            article_url = f\"https://www.techinasia.com{href}\" if not href.startswith('http') else href\n",
    "            match = re.search(r'/([^/]+)$', href)\n",
    "            article_id = match.group(1) if match else None\n",
    "            logger.info(f\"  - Article URL extracted: {article_url}, ID: {article_id}\")\n",
    "        else:\n",
    "            logger.warning(f\"  - No article links found.\")\n",
    "\n",
    "        # Get source information\n",
    "        logger.info(f\"  - Extracting source... üì∞\")\n",
    "        source_element = content_div.find('span', class_='post-source-name')\n",
    "        source = source_element.text.strip() if source_element else None\n",
    "        logger.info(f\"  - Source extracted: {source} ‚úÖ\")\n",
    "\n",
    "        source_link = content_div.find('a', class_='post-source')\n",
    "        source_url = source_link.get('href') if source_link else None\n",
    "        logger.info(f\"  - Source URL extracted: {source_url} üåê\")\n",
    "\n",
    "        # Get image information üñºÔ∏è\n",
    "        logger.info(f\"  - Extracting image URL... üñºÔ∏è\")\n",
    "        image_div = article_element.find('div', class_='post-image')\n",
    "        if image_div:\n",
    "            img_tag = image_div.find('img')\n",
    "            image_url = img_tag.get('src') if img_tag else None\n",
    "            logger.info(f\"  - Image URL extracted: {image_url} üñºÔ∏è\")\n",
    "        else:\n",
    "            logger.warning(f\"  - No image div found. üñºÔ∏è\")\n",
    "\n",
    "        # Get time and categories/tags\n",
    "        logger.info(f\"  - Extracting time and categories/tags... ‚è∞\")\n",
    "        footer = article_element.find('div', class_='post-footer')\n",
    "        time_element = footer.find('time') if footer else None\n",
    "        posted_time = time_element.get('datetime') if time_element else None\n",
    "        relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "        logger.info(f\"  - Time extracted: {posted_time} ‚è∞, Relative Time: {relative_time}\")\n",
    "\n",
    "        # Parse categories and tags\n",
    "        if footer:\n",
    "            tag_elements = footer.find_all('a', class_='post-taxonomy-link')\n",
    "            for tag in tag_elements:\n",
    "                tag_text = tag.text.strip('¬∑ ')\n",
    "                if tag_text:\n",
    "                    if tag.get('href', '').startswith('/category/'):\n",
    "                        categories.append(tag_text)\n",
    "                    elif tag.get('href', '').startswith('/tag/'):\n",
    "                        tags.append(tag_text)\n",
    "            logger.info(f\"  - Categories: {categories}, Tags: {tags}\")\n",
    "\n",
    "        article = Article(\n",
    "            article_id=article_id,\n",
    "            title=title,\n",
    "            article_url=article_url,\n",
    "            source=source,\n",
    "            source_url=source_url,\n",
    "            image_url=image_url,\n",
    "            posted_time=posted_time,\n",
    "            relative_time=relative_time,\n",
    "            categories=categories,\n",
    "            tags=tags,\n",
    "        )\n",
    "        logger.info(f\"Article parsing complete: {article_id} üéâ\")\n",
    "        return article\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Error parsing article: {article_id}, error: {e}\")\n",
    "        return None"
   ],
   "id": "c2ba31cd8f2a2e26",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Scraping Logic\n",
    "\n",
    "This section outlines the main scraping process:\n",
    "\n",
    "-   **Main Scraping Method**: The `scrape` method orchestrates the entire scraping process, including setting up the WebDriver, performing the scraping, and processing the scraped data.\n",
    "-   **Retry Mechanism**: The `_scrape_with_retry` method implements a retry mechanism to handle potential errors during scraping. It retries the scraping process up to a specified number of times with exponential backoff.\n",
    "-   **Performing Scraping**: The `_perform_scraping` method navigates to the target URL, scrolls the page to load more articles, and parses each article element. It uses a progress bar to track the scraping progress.\n",
    "-   **Data Processing**: The `_process_articles` method converts the scraped articles into a pandas DataFrame, removes duplicates, and saves the data in batches as CSV files."
   ],
   "id": "573d5d615b4e126a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:51.684737Z",
     "start_time": "2025-01-15T03:30:51.673197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code Cell\n",
    "def scrape(self) -> pd.DataFrame:\n",
    "    \"\"\"Main scraping method\"\"\"\n",
    "    articles = []\n",
    "    try:\n",
    "        self.setup_driver()\n",
    "        articles = self._scrape_with_retry()\n",
    "        return self._process_articles(articles)\n",
    "    finally:\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "def _scrape_with_retry(self) -> List[Article]:\n",
    "    \"\"\"Implement retry logic for scraping\"\"\"\n",
    "    for attempt in range(self.config.retry_count):\n",
    "        try:\n",
    "            return self._perform_scraping()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed: {e}, retrying...\")\n",
    "            if attempt == self.config.retry_count - 1:\n",
    "                raise\n",
    "            time.sleep(2 * (attempt + 1))  # Exponential backoff\n",
    "\n",
    "def _perform_scraping(self) -> List[Article]:\n",
    "    \"\"\"Perform the actual scraping\"\"\"\n",
    "    articles = []\n",
    "    url = f\"{self.config.base_url}?category=artificial-intelligence\"\n",
    "\n",
    "    logger.info(f\"Navigating to URL: {url}\")\n",
    "    self.driver.get(url)\n",
    "    wait = WebDriverWait(self.driver, self.config.timeout)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'post-card')))\n",
    "    logger.info(f\"Page loaded successfully.\")\n",
    "\n",
    "    progress_bar = tqdm(total=self.config.num_articles, desc=\"Scraping Articles\", unit=\"article\")\n",
    "    scroll_count = 0\n",
    "    while len(articles) < self.config.num_articles and scroll_count < self.config.max_scrolls:\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        article_elements = soup.find_all('article', class_='post-card')\n",
    "\n",
    "        for article_element in article_elements:\n",
    "            try:\n",
    "                article = self.parse_article(article_element)\n",
    "                if article and article.article_id not in self.processed_article_ids:\n",
    "                    if self._is_valid_article(article):\n",
    "                        cleaned_article = self._clean_article_data(article)\n",
    "                        articles.append(cleaned_article)\n",
    "                        self.processed_article_ids.add(article.article_id)\n",
    "                        logger.info(f\"Processed article {article.article_id} - {article.title}\")\n",
    "                        progress_bar.update(1)\n",
    "                        progress_bar.set_postfix({\"incomplete\": self.incomplete_articles})\n",
    "                    else:\n",
    "                        self.incomplete_articles += 1\n",
    "                    self.total_articles += 1\n",
    "                    if len(articles) >= self.config.num_articles:\n",
    "                        break\n",
    "            except StaleElementReferenceException as e:\n",
    "                logger.warning(f\"StaleElementReferenceException: {e}, retrying parsing\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during article processing: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not self.scroll_manager.scroll_page():\n",
    "            break\n",
    "        scroll_count += 1\n",
    "        logger.info(f\"Scrolled {scroll_count} times, found {len(articles)} articles\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    return articles\n",
    "\n",
    "def _process_articles(self, articles: List[Article]) -> pd.DataFrame:\n",
    "    \"\"\"Process and clean scraped articles\"\"\"\n",
    "    if not articles:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert articles to DataFrame\n",
    "    df = pd.DataFrame([article.to_dict() for article in articles])\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['article_url'], keep='first')\n",
    "\n",
    "    # Save in batches\n",
    "    self._save_batches(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def _save_batches(self, df: pd.DataFrame):\n",
    "    \"\"\"Save DataFrame in batches\"\"\"\n",
    "    batch_size = self.config.batch_size\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df[i:i + batch_size]\n",
    "        filename = f\"{self.config.output_dir}/techinasia_ai_news_v1_batch_{i//batch_size}_{timestamp}.csv\"\n",
    "        try:\n",
    "            batch.to_csv(filename, index=False)\n",
    "            logger.info(f\"Saved batch {i//batch_size + 1} to {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving batch {i//batch_size + 1} to {filename}: {e}\")"
   ],
   "id": "9c1e970ad7bfc7b6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Execution and Summary\n",
    "\n",
    "This section executes the scraper and provides a summary of the scraping process:\n",
    "\n",
    "-   **Main Function**: The `main` function parses command-line arguments, initializes the `TechInAsiaScraper`, and executes the scraping process.\n",
    "-   **Sample Display**: After scraping, a sample of the scraped articles is displayed, including the title, source, posted time, categories, and tags.\n",
    "-   **Summary Logging**: The `_log_summary` method logs summary statistics of the scraping process, including the total number of articles scraped, valid articles, incomplete articles, and duplicate articles."
   ],
   "id": "3000b5398405063b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:30:52.774653Z",
     "start_time": "2025-01-15T03:30:52.767793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code Cell\n",
    "def _log_summary(self):\n",
    "    \"\"\"Log summary statistics of the scraping process\"\"\"\n",
    "    logger.info(\"üìä --- Scraping Summary ---\")\n",
    "    logger.info(f\"üì∞ Total articles scraped: {self.total_articles}\")\n",
    "    logger.info(f\"‚úÖ Valid articles scraped: {len(self.processed_article_ids)}\")\n",
    "    logger.info(f\"‚ö†Ô∏è Incomplete articles skipped: {self.incomplete_articles}\")\n",
    "    logger.info(f\"üîÑ Duplicate articles skipped: {self.total_articles - len(self.processed_article_ids) - self.incomplete_articles}\")\n",
    "    logger.info(\"üìä ------------------------\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"TechInAsia Scraper\")\n",
    "    parser.add_argument('--num_articles', type=int, help='Number of articles to scrape', default=100)\n",
    "    parser.add_argument('--max_scrolls', type=int, help='Maximum number of scrolls', default=15)\n",
    "    parser.add_argument('--output_dir', type=str, help='Output directory', default='techinasia_output')\n",
    "    parser.add_argument('--min_delay', type=float, help='Minimum delay between scrolls', default=1)\n",
    "    parser.add_argument('--max_delay', type=float, help='Maximum delay between scrolls', default=3)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = {\n",
    "        'num_articles': args.num_articles,\n",
    "        'max_scrolls': args.max_scrolls,\n",
    "        'output_dir': args.output_dir,\n",
    "        'min_delay': args.min_delay,\n",
    "        'max_delay': args.max_delay\n",
    "    }\n",
    "\n",
    "    logger.info(\"Starting TechInAsia scraper v1.5\")\n",
    "    scraper = TechInAsiaScraper(config)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape()\n",
    "        logger.info(f\"Successfully scraped {len(df)} articles\")\n",
    "\n",
    "        # Display sample of results\n",
    "        if not df.empty:\n",
    "            print(\"\\nSample of scraped articles:\")\n",
    "            display_columns = ['title', 'source', 'posted_time_iso', 'categories', 'tags']\n",
    "            print(df[display_columns].head())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {e}\")\n",
    "    finally:\n",
    "        scraper._log_summary()\n",
    "\n"
   ],
   "id": "51811fc1af27f69",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T03:35:03.581134Z",
     "start_time": "2025-01-15T03:34:54.283079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code Cell - Test Run\n",
    "\n",
    "import logging\n",
    "from techinasia_scraper_v1_5 import TechInAsiaScraper  # Assuming your script is named techinasia_scraper_v1_5.py\n",
    "\n",
    "# Configure logging for this test run\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define a test configuration\n",
    "test_config = {\n",
    "    'num_articles': 75,  # Scrape only 5 articles for a quick test\n",
    "    'max_scrolls': 3,   # Limit the number of scrolls\n",
    "    'output_dir': 'techinasia_output',  # Use a specific output directory for testing\n",
    "    'min_delay': 0.5,\n",
    "    'max_delay': 1\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Initialize the scraper with the test configuration\n",
    "    scraper = TechInAsiaScraper(test_config)\n",
    "\n",
    "    # Run the scraper\n",
    "    df = scraper.scrape()\n",
    "\n",
    "    # Check if any articles were scraped\n",
    "    if not df.empty:\n",
    "        print(f\"‚úÖ Test run successful! Scraped {len(df)} articles.\")\n",
    "        print(\"\\nSample of scraped articles:\")\n",
    "        display_columns = ['title', 'source', 'posted_time_iso', 'categories', 'tags']\n",
    "        print(df[display_columns].head())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Test run completed, but no articles were scraped.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Test run failed: {e}\")\n",
    "    print(f\"‚ùå Test run failed: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'scraper' in locals():\n",
    "        scraper._log_summary()"
   ],
   "id": "77e541f3d862aa78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Articles:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 45/75 [00:03<00:02, 14.94article/s, incomplete=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test run successful! Scraped 45 articles.\n",
      "\n",
      "Sample of scraped articles:\n",
      "                                               title     source  \\\n",
      "0  Ytl, Sea launch Malaysia‚Äôs first AI-powered di...   Ryt Bank   \n",
      "1    Taiwanese startup MetAI raises $4m seed funding      MetAI   \n",
      "2  Saudi firm Halo AI secures $6m for global expa...  Arab News   \n",
      "3  Israeli cybersecurity startup raises $36m seed...  Calcalist   \n",
      "4  Biden signs order for AI data centers, US-made...       CNBC   \n",
      "\n",
      "       posted_time_iso      categories      tags  \n",
      "0  2025-01-15T11:08:00      AI,Fintech  Malaysia  \n",
      "1  2025-01-15T11:00:00  AI,Investments            \n",
      "2  2025-01-15T10:05:00  AI,Investments            \n",
      "3  2025-01-15T10:00:00  AI,Investments            \n",
      "4  2025-01-15T09:00:00              AI            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a1a5f1a65ee81548"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
