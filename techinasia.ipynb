{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T12:24:32.964855Z",
     "start_time": "2025-01-14T12:24:32.342078Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# For handling request errors\n",
    "from requests.exceptions import RequestException"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:24:56.559221Z",
     "start_time": "2025-01-14T12:24:56.555386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "    }"
   ],
   "id": "d6cacde578e75c28",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_techinasia(page_number=1):\n",
    "    base_url = f'https://www.techinasia.com/news'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        # Add delay to be respectful to the website\n",
    "        time.sleep(2)\n",
    "\n",
    "        response = requests.get(base_url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements (you'll need to inspect the website to get the correct CSS selectors)\n",
    "        article_elements = soup.find_all('article', class_='jsx-1786728215')\n",
    "\n",
    "        for article in article_elements:\n",
    "            try:\n",
    "                title = article.find('h2').text.strip()\n",
    "                link = article.find('a')['href']\n",
    "                date = article.find('time')['datetime']\n",
    "\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'link': f'https://www.techinasia.com{link}',\n",
    "                    'date': date,\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "            except (AttributeError, KeyError) as e:\n",
    "                print(f\"Error parsing article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return articles\n",
    "\n",
    "    except RequestException as e:\n",
    "        print(f\"Error fetching page {page_number}: {e}\")\n",
    "        return []"
   ],
   "id": "4b54d646c22996c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76577cf546487b8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:29:03.738951Z",
     "start_time": "2025-01-14T12:29:03.730966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "    }\n",
    "\n",
    "def scrape_techinasia(page_number=1):\n",
    "    base_url = f'https://www.techinasia.com/news?page={page_number}'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        time.sleep(2)  # Respectful delay\n",
    "\n",
    "        response = requests.get(base_url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements with the correct class\n",
    "        article_elements = soup.find_all('article', class_='jsx-1678928787 post-card')\n",
    "\n",
    "        for article in article_elements:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title = article.find('h3', class_='post-title').text.strip()\n",
    "\n",
    "                # Extract source\n",
    "                source_element = article.find('span', class_='post-source-name')\n",
    "                source = source_element.text if source_element else 'N/A'\n",
    "\n",
    "                # Extract time\n",
    "                time_element = article.find('time')\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "\n",
    "                # Extract categories/tags\n",
    "                tags = []\n",
    "                tag_elements = article.find_all('a', class_='post-taxonomy-link')\n",
    "                for tag in tag_elements:\n",
    "                    tags.append(tag.text.strip('· '))\n",
    "\n",
    "                # Extract article URL\n",
    "                link = article.find('h3', class_='post-title').find_parent('a')['href']\n",
    "                full_link = f'https://www.techinasia.com{link}'\n",
    "\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'source': source,\n",
    "                    'posted_time': posted_time,\n",
    "                    'tags': ', '.join(tags),\n",
    "                    'url': full_link,\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "\n",
    "            except (AttributeError, KeyError) as e:\n",
    "                print(f\"Error parsing article: {e}\")\n",
    "                continue\n",
    "\n",
    "        return articles\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page {page_number}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to scrape multiple pages\n",
    "def scrape_multiple_pages(start_page=1, end_page=5):\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        articles = scrape_techinasia(page)\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "    return pd.DataFrame(all_articles)"
   ],
   "id": "c76fe6512c5ec1c8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:29:11.939429Z",
     "start_time": "2025-01-14T12:29:04.638196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scrape first 3 pages\n",
    "df = scrape_multiple_pages(1, 3)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('techinasia_news.csv', index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())"
   ],
   "id": "1a51c6c133bb1e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a4af18e57f2b78f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5bbb00a078d5f6a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a544f66161bb9222"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:34:56.649980Z",
     "start_time": "2025-01-14T12:34:56.640167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.techinasia.com/'\n",
    "    }\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=20):\n",
    "    # Initial URL for AI category\n",
    "    url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all article elements\n",
    "        article_elements = soup.find_all('article', class_='jsx-1678928787 post-card')\n",
    "\n",
    "        for article in article_elements:\n",
    "            try:\n",
    "                # Extract source information\n",
    "                source_element = article.find('span', class_='post-source-name')\n",
    "                source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "                # Extract source URL\n",
    "                source_link = article.find('a', class_='post-source')['href'] if article.find('a', class_='post-source') else None\n",
    "\n",
    "                # Extract title\n",
    "                title_element = article.find('h3', class_='post-title')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "                # Extract TechInAsia article URL\n",
    "                article_url = None\n",
    "                title_link = article.find('h3', class_='post-title').find_parent('a')\n",
    "                if title_link:\n",
    "                    article_url = f\"https://www.techinasia.com{title_link['href']}\"\n",
    "\n",
    "                # Extract time\n",
    "                time_element = article.find('time')\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "                relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "                # Extract categories and tags\n",
    "                tags = []\n",
    "                category_elements = article.find_all('a', class_='post-taxonomy-link')\n",
    "                for tag in category_elements:\n",
    "                    tags.append(tag.text.strip('· '))\n",
    "\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'source': source,\n",
    "                    'source_url': source_link,\n",
    "                    'article_url': article_url,\n",
    "                    'posted_time': posted_time,\n",
    "                    'relative_time': relative_time,\n",
    "                    'tags': ', '.join(tags),\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(articles) >= num_articles:\n",
    "                break\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page: {e}\")\n",
    "        return []\n",
    "\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "# Function to save the results\n",
    "def save_results(df):\n",
    "    # Save to CSV\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(df)} articles to {filename}\")\n",
    "\n",
    "    # Display first few articles\n",
    "    print(\"\\nFirst few articles:\")\n",
    "    print(df[['title', 'source', 'posted_time']].head())\n",
    "\n",
    "    return filename"
   ],
   "id": "acd5067ff6dd27e4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:35:01.316959Z",
     "start_time": "2025-01-14T12:34:59.759998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scrape the latest 20 AI news articles\n",
    "df = scrape_techinasia_ai_news(20)\n",
    "\n",
    "# Save and display results\n",
    "filename = save_results(df)"
   ],
   "id": "45f5c0c11552a2a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 articles to techinasia_ai_news_20250114_203500.csv\n",
      "\n",
      "First few articles:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['title', 'source', 'posted_time'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m scrape_techinasia_ai_news(\u001B[38;5;241m20\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Save and display results\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m filename \u001B[38;5;241m=\u001B[39m \u001B[43msave_results\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 93\u001B[0m, in \u001B[0;36msave_results\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;66;03m# Display first few articles\u001B[39;00m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFirst few articles:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 93\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtitle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msource\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mposted_time\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mhead())\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m filename\n",
      "File \u001B[0;32m~/gilzero.dev/project-ai-news-aggregator/venv/lib/python3.13/site-packages/pandas/core/frame.py:4108\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[1;32m   4107\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 4108\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   4110\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[1;32m   4111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[0;32m~/gilzero.dev/project-ai-news-aggregator/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[0;34m(self, key, axis_name)\u001B[0m\n\u001B[1;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[1;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[1;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[0;32m~/gilzero.dev/project-ai-news-aggregator/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6249\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[0;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[1;32m   6247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nmissing:\n\u001B[1;32m   6248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nmissing \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[0;32m-> 6249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6251\u001B[0m     not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m   6252\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"None of [Index(['title', 'source', 'posted_time'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2ff36fee2f029f2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b0ec34d2b095bf46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:36:20.760206Z",
     "start_time": "2025-01-14T12:36:20.336660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.techinasia.com/'\n",
    "    }\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=20):\n",
    "    url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching URL:\", url)\n",
    "        response = requests.get(url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main container\n",
    "        main_container = soup.find('section', class_='jsx-2115179455 news-section')\n",
    "        if not main_container:\n",
    "            print(\"Could not find main news container\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Find all article elements\n",
    "        article_elements = main_container.find_all('article', class_='jsx-1678928787 post-card')\n",
    "        print(f\"Found {len(article_elements)} articles\")\n",
    "\n",
    "        for i, article in enumerate(article_elements, 1):\n",
    "            try:\n",
    "                # Extract title and its link\n",
    "                title_element = article.find('h3', class_='post-title')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "                # Get the complete article URL\n",
    "                article_link = article.find('h3', class_='post-title').find_parent('a')\n",
    "                article_url = f\"https://www.techinasia.com{article_link['href']}\" if article_link else None\n",
    "\n",
    "                # Get source information\n",
    "                source_element = article.find('span', class_='post-source-name')\n",
    "                source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "                source_link = article.find('a', class_='post-source')\n",
    "                source_url = source_link['href'] if source_link else None\n",
    "\n",
    "                # Get time information\n",
    "                time_element = article.find('time', class_='post-time')\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "                relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "                # Get categories and tags\n",
    "                tags = []\n",
    "                tag_elements = article.find_all('a', class_='post-taxonomy-link')\n",
    "                for tag in tag_elements:\n",
    "                    tag_text = tag.text.strip('· ')\n",
    "                    if tag_text:\n",
    "                        tags.append(tag_text)\n",
    "\n",
    "                article_data = {\n",
    "                    'title': title,\n",
    "                    'article_url': article_url,\n",
    "                    'source': source,\n",
    "                    'source_url': source_url,\n",
    "                    'posted_time': posted_time,\n",
    "                    'relative_time': relative_time,\n",
    "                    'tags': ', '.join(tags) if tags else 'N/A',\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "\n",
    "                print(f\"Processing article {i}: {title[:50]}...\")\n",
    "                articles.append(article_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article {i}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            if len(articles) >= num_articles:\n",
    "                break\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles were collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    print(f\"\\nSuccessfully collected {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def save_results(df):\n",
    "    if df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return None\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSaved {len(df)} articles to {filename}\")\n",
    "\n",
    "    # Display first few articles\n",
    "    print(\"\\nFirst few articles:\")\n",
    "    print(df[['title', 'source', 'posted_time']].head())\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Test the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting scraper...\")\n",
    "    df = scrape_techinasia_ai_news(20)\n",
    "    if not df.empty:\n",
    "        save_results(df)"
   ],
   "id": "c64c5fee0e1f3609",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper...\n",
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "Could not find main news container\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:36:34.719646Z",
     "start_time": "2025-01-14T12:36:34.284648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = scrape_techinasia_ai_news(20)\n",
    "save_results(df)"
   ],
   "id": "f6c42f59c6d11074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "Could not find main news container\n",
      "No data to save\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:37:15.171216Z",
     "start_time": "2025-01-14T12:37:14.732133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.techinasia.com/'\n",
    "    }\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=20):\n",
    "    url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching URL:\", url)\n",
    "        response = requests.get(url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Debug: Print the HTML structure\n",
    "        print(\"\\nSearching for news container...\")\n",
    "\n",
    "        # First find the post-list-wrapper\n",
    "        post_list = soup.find('div', class_='jsx-4205048050 post-list-wrapper')\n",
    "        if not post_list:\n",
    "            print(\"Could not find post list wrapper\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Find all article elements\n",
    "        article_elements = post_list.find_all('article', class_='jsx-1678928787')\n",
    "        print(f\"Found {len(article_elements)} articles\")\n",
    "\n",
    "        for i, article in enumerate(article_elements, 1):\n",
    "            try:\n",
    "                # Get post content div\n",
    "                content_div = article.find('div', class_='jsx-1678928787 post-content')\n",
    "\n",
    "                # Extract title and its link\n",
    "                title_element = content_div.find('h3', class_='jsx-1678928787 post-title')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "                # Get the complete article URL\n",
    "                article_link = content_div.find('a', href=True)\n",
    "                article_url = f\"https://www.techinasia.com{article_link['href']}\" if article_link and not article_link['href'].startswith('http') else article_link['href']\n",
    "\n",
    "                # Get source information\n",
    "                source_element = content_div.find('span', class_='jsx-1678928787 post-source-name')\n",
    "                source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "                source_link = content_div.find('a', class_='jsx-1678928787 post-source')\n",
    "                source_url = source_link['href'] if source_link else None\n",
    "\n",
    "                # Get time information from post footer\n",
    "                footer = article.find('div', class_='jsx-1678928787 post-footer')\n",
    "                time_element = footer.find('time') if footer else None\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "                relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "                # Get categories and tags from footer\n",
    "                tags = []\n",
    "                if footer:\n",
    "                    tag_elements = footer.find_all('a', class_='post-taxonomy-link')\n",
    "                    for tag in tag_elements:\n",
    "                        tag_text = tag.text.strip('· ')\n",
    "                        if tag_text:\n",
    "                            tags.append(tag_text)\n",
    "\n",
    "                article_data = {\n",
    "                    'title': title,\n",
    "                    'article_url': article_url,\n",
    "                    'source': source,\n",
    "                    'source_url': source_url,\n",
    "                    'posted_time': posted_time,\n",
    "                    'relative_time': relative_time,\n",
    "                    'tags': ', '.join(tags) if tags else 'N/A',\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "\n",
    "                print(f\"Processing article {i}: {title[:50]}...\")\n",
    "                articles.append(article_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article {i}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            if len(articles) >= num_articles:\n",
    "                break\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles were collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    print(f\"\\nSuccessfully collected {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def save_results(df):\n",
    "    if df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return None\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSaved {len(df)} articles to {filename}\")\n",
    "\n",
    "    # Display first few articles\n",
    "    print(\"\\nFirst few articles:\")\n",
    "    print(df[['title', 'source', 'posted_time']].head())\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Test the scraper\n",
    "df = scrape_techinasia_ai_news(20)\n",
    "save_results(df)"
   ],
   "id": "13bb5ae4df64e637",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "\n",
      "Searching for news container...\n",
      "Could not find post list wrapper\n",
      "No data to save\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:38:01.927039Z",
     "start_time": "2025-01-14T12:38:01.494676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.techinasia.com/'\n",
    "    }\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=20):\n",
    "    url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching URL:\", url)\n",
    "        response = requests.get(url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        print(\"\\nSearching for news container...\")\n",
    "\n",
    "        # Debug: Print initial structure\n",
    "        main_section = soup.find('main')\n",
    "        if not main_section:\n",
    "            print(\"Could not find main section\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # First find the infinite scroll container\n",
    "        infinite_scroll = soup.find('div', class_='infinite-scroll')\n",
    "        if not infinite_scroll:\n",
    "            print(\"Could not find infinite scroll container\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Find post list wrapper (using partial class match)\n",
    "        post_list = infinite_scroll.find('div', class_=lambda x: x and 'post-list-wrapper' in x)\n",
    "        if not post_list:\n",
    "            print(\"Could not find post list wrapper\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Find all article elements (using partial class match)\n",
    "        article_elements = post_list.find_all('article', class_=lambda x: x and 'post-card' in x)\n",
    "        print(f\"Found {len(article_elements)} articles\")\n",
    "\n",
    "        for i, article in enumerate(article_elements, 1):\n",
    "            try:\n",
    "                # Get post content div\n",
    "                content_div = article.find('div', class_=lambda x: x and 'post-content' in x)\n",
    "                if not content_div:\n",
    "                    print(f\"No content div found for article {i}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract title and its link\n",
    "                title_element = content_div.find('h3', class_=lambda x: x and 'post-title' in x)\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "                # Get article URL\n",
    "                article_link = content_div.find_all('a')[-1] if content_div.find_all('a') else None\n",
    "                article_url = f\"https://www.techinasia.com{article_link['href']}\" if article_link and not article_link['href'].startswith('http') else article_link['href'] if article_link else None\n",
    "\n",
    "                # Get source information\n",
    "                source_element = content_div.find('span', class_=lambda x: x and 'post-source-name' in x)\n",
    "                source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "                source_link = content_div.find('a', class_=lambda x: x and 'post-source' in x)\n",
    "                source_url = source_link['href'] if source_link else None\n",
    "\n",
    "                # Get time information from post footer\n",
    "                footer = article.find('div', class_=lambda x: x and 'post-footer' in x)\n",
    "                time_element = footer.find('time') if footer else None\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "                relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "                # Get categories and tags\n",
    "                tags = []\n",
    "                if footer:\n",
    "                    tag_elements = footer.find_all('a', class_='post-taxonomy-link')\n",
    "                    for tag in tag_elements:\n",
    "                        tag_text = tag.text.strip('· ')\n",
    "                        if tag_text:\n",
    "                            tags.append(tag_text)\n",
    "\n",
    "                article_data = {\n",
    "                    'title': title,\n",
    "                    'article_url': article_url,\n",
    "                    'source': source,\n",
    "                    'source_url': source_url,\n",
    "                    'posted_time': posted_time,\n",
    "                    'relative_time': relative_time,\n",
    "                    'tags': ', '.join(tags) if tags else 'N/A',\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "\n",
    "                print(f\"Processing article {i}: {title[:50]}...\")\n",
    "                articles.append(article_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article {i}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            if len(articles) >= num_articles:\n",
    "                break\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles were collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    print(f\"\\nSuccessfully collected {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def save_results(df):\n",
    "    if df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return None\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSaved {len(df)} articles to {filename}\")\n",
    "\n",
    "    # Display first few articles\n",
    "    print(\"\\nFirst few articles:\")\n",
    "    print(df[['title', 'source', 'posted_time']].head())\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Debug: Print the actual HTML\n",
    "df = scrape_techinasia_ai_news(20)\n",
    "save_results(df)"
   ],
   "id": "f705871948e3b340",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "\n",
      "Searching for news container...\n",
      "Could not find main section\n",
      "No data to save\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:38:42.872667Z",
     "start_time": "2025-01-14T12:38:42.414187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.techinasia.com/'\n",
    "    }\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=20):\n",
    "    url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "    articles = []\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching URL:\", url)\n",
    "        response = requests.get(url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Debug: Print the first part of the response\n",
    "        print(\"\\nFirst 1000 characters of response:\")\n",
    "        print(response.text[:1000])\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Debug: Print all main tags found\n",
    "        print(\"\\nAll main tags found:\")\n",
    "        main_tags = soup.find_all('main')\n",
    "        for i, main in enumerate(main_tags):\n",
    "            print(f\"Main tag {i+1} classes:\", main.get('class', 'No class'))\n",
    "\n",
    "        # Debug: Print all div tags with 'infinite-scroll' in their class\n",
    "        print(\"\\nAll infinite-scroll divs found:\")\n",
    "        infinite_divs = soup.find_all('div', class_=lambda x: x and 'infinite-scroll' in x)\n",
    "        for i, div in enumerate(infinite_divs):\n",
    "            print(f\"Infinite scroll div {i+1} classes:\", div.get('class', 'No class'))\n",
    "\n",
    "        # Debug: Print all article tags\n",
    "        print(\"\\nAll article tags found:\")\n",
    "        articles_found = soup.find_all('article')\n",
    "        print(f\"Found {len(articles_found)} article tags\")\n",
    "\n",
    "        if len(articles_found) == 0:\n",
    "            print(\"\\nLooks like the page content is loaded via JavaScript. We need to use Selenium.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Rest of the code...\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles were collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    print(f\"\\nSuccessfully collected {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def save_results(df):\n",
    "    if df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return None\n",
    "\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSaved {len(df)} articles to {filename}\")\n",
    "\n",
    "    print(\"\\nFirst few articles:\")\n",
    "    print(df[['title', 'source', 'posted_time']].head())\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Run the scraper with debug output\n",
    "df = scrape_techinasia_ai_news(20)\n",
    "save_results(df)"
   ],
   "id": "decbba94016bb7a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "\n",
      "First 1000 characters of response:\n",
      "<!DOCTYPE html><html lang=\"en-US\" itemscope itemtype=\"http://schema.org/WebSite\" prefix=\"og: http://ogp.me/ns#\"><head><style>#zmmtg-root {\n",
      "        display: none;\n",
      "      }</style><title>Tech in Asia - Connecting Asia's startup ecosystem</title><meta charset=\"utf-8\"><meta http-equiv=\"x-dns-prefetch-control\" content=\"on\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,viewport-fit=cover\"><meta name=\"msapplication-config\" content=\"https://static.techinasia.com/assets/browserconfig.xml\"><meta name=\"theme-color\" content=\"#ffffff\"><meta name=\"twitter:widgets:csp\" content=\"on\"><meta name=\"fragment\" content=\"!\"><meta property=\"fb:app_id\" content=\"206930646126140\"><meta property=\"fb:pages\" content=\"175755689129519\"><meta property=\"og:locale\" content=\"en_US\"><link rel=\"dns-prefetch\" href=\"//static.techinasia.com\"><link rel=\"dns-prefetch\" href=\"//cdn.techinasia.com\"><link rel=\"dns-prefetch\" href=\"//d1h69ey09xg1xv.cloudfront.net\"\n",
      "\n",
      "All main tags found:\n",
      "\n",
      "All infinite-scroll divs found:\n",
      "\n",
      "All article tags found:\n",
      "Found 0 article tags\n",
      "\n",
      "Looks like the page content is loaded via JavaScript. We need to use Selenium.\n",
      "No data to save\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:40:09.487414Z",
     "start_time": "2025-01-14T12:39:59.198558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=20):\n",
    "    articles = []\n",
    "\n",
    "    # Setup Chrome options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run in headless mode\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    # Add user agent\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "\n",
    "    try:\n",
    "        print(\"Starting Chrome webdriver...\")\n",
    "        # Use webdriver_manager to handle driver installation\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "        print(f\"Fetching URL: {url}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        print(\"Waiting for content to load...\")\n",
    "\n",
    "        # Wait for articles to load (maximum 20 seconds)\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        article_present = wait.until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'post-card'))\n",
    "        )\n",
    "\n",
    "        # Give extra time for all articles to load\n",
    "        time.sleep(3)\n",
    "\n",
    "        print(\"Page loaded, parsing content...\")\n",
    "\n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        article_elements = soup.find_all('article', class_='post-card')\n",
    "        print(f\"Found {len(article_elements)} articles\")\n",
    "\n",
    "        for i, article in enumerate(article_elements, 1):\n",
    "            try:\n",
    "                # Extract article information\n",
    "                content_div = article.find('div', class_='post-content')\n",
    "\n",
    "                # Get title\n",
    "                title_element = content_div.find('h3', class_='post-title')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "                # Get source info\n",
    "                source_element = content_div.find('span', class_='post-source-name')\n",
    "                source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "                source_link = content_div.find('a', class_='post-source')\n",
    "                source_url = source_link['href'] if source_link else None\n",
    "\n",
    "                # Get article URL\n",
    "                article_links = [a for a in content_div.find_all('a') if not 'post-source' in a.get('class', [])]\n",
    "                article_url = None\n",
    "                if article_links:\n",
    "                    href = article_links[0]['href']\n",
    "                    article_url = f\"https://www.techinasia.com{href}\" if not href.startswith('http') else href\n",
    "\n",
    "                # Get time and tags from footer\n",
    "                footer = article.find('div', class_='post-footer')\n",
    "                time_element = footer.find('time') if footer else None\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "                relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "                # Get tags\n",
    "                tags = []\n",
    "                if footer:\n",
    "                    tag_elements = footer.find_all('a', class_='post-taxonomy-link')\n",
    "                    tags = [tag.text.strip('· ') for tag in tag_elements if tag.text.strip('· ')]\n",
    "\n",
    "                article_data = {\n",
    "                    'title': title,\n",
    "                    'article_url': article_url,\n",
    "                    'source': source,\n",
    "                    'source_url': source_url,\n",
    "                    'posted_time': posted_time,\n",
    "                    'relative_time': relative_time,\n",
    "                    'tags': ', '.join(tags) if tags else 'N/A',\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "\n",
    "                print(f\"Processing article {i}: {title[:50]}...\")\n",
    "                articles.append(article_data)\n",
    "\n",
    "                if len(articles) >= num_articles:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article {i}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"Closing browser...\")\n",
    "        driver.quit()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles were collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    print(f\"\\nSuccessfully collected {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def save_results(df):\n",
    "    if df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return None\n",
    "\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSaved {len(df)} articles to {filename}\")\n",
    "\n",
    "    print(\"\\nFirst few articles:\")\n",
    "    print(df[['title', 'source', 'posted_time']].head())\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Run the scraper\n",
    "print(\"Starting scraper...\")\n",
    "df = scrape_techinasia_ai_news(20)\n",
    "save_results(df)"
   ],
   "id": "fa6cf619985a12ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraper...\n",
      "Starting Chrome webdriver...\n",
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "Waiting for content to load...\n",
      "Page loaded, parsing content...\n",
      "Found 15 articles\n",
      "Processing article 1: AWS, General Catalyst advance AI tools to support ...\n",
      "Processing article 2: Trump likely to keep US AI chip export restriction...\n",
      "Processing article 3: OpenAI urges action to maintain US AI leadership...\n",
      "Processing article 4: TSMC Q4 profit set to surge 58% on strong AI chip ...\n",
      "Processing article 5: PUBG maker Krafton to invest $136m in game studios...\n",
      "Processing article 6: Microsoft forms new engineering group to boost AI ...\n",
      "Processing article 7: US tighten AI chip exports, targets China...\n",
      "Processing article 8: Indonesian AI character generator nets $5m seed...\n",
      "Processing article 9: Google’s AI agent enhances voice assist in Mercede...\n",
      "Processing article 10: Adobe launches new AI tool for bulk image editing...\n",
      "Processing article 11: Chinese AI startup DeepSeek launches free iOS app...\n",
      "Processing article 12: South Korea boosts AI, semiconductor ties with Isr...\n",
      "Processing article 13: SK Telecom launches GPUaaS for South Korea’s AI da...\n",
      "Processing article 14: YouTubers sell unused video footage to AI firms...\n",
      "Processing article 15: Singtel launches Perplexity Pro AI with free year ...\n",
      "Closing browser...\n",
      "\n",
      "Successfully collected 15 articles\n",
      "\n",
      "Saved 15 articles to techinasia_ai_news_20250114_204009.csv\n",
      "\n",
      "First few articles:\n",
      "                                               title     source  \\\n",
      "0  AWS, General Catalyst advance AI tools to supp...       CNBC   \n",
      "1  Trump likely to keep US AI chip export restric...  Bloomberg   \n",
      "2   OpenAI urges action to maintain US AI leadership    Reuters   \n",
      "3  TSMC Q4 profit set to surge 58% on strong AI c...    Reuters   \n",
      "4  PUBG maker Krafton to invest $136m in game stu...  Bloomberg   \n",
      "\n",
      "               posted_time  \n",
      "0  6:05 PM at Jan 14, 2025  \n",
      "1  5:56 PM at Jan 14, 2025  \n",
      "2  3:38 PM at Jan 14, 2025  \n",
      "3  3:15 PM at Jan 14, 2025  \n",
      "4  1:02 PM at Jan 14, 2025  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'techinasia_ai_news_20250114_204009.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff7a38637170477d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "enhanced version\n",
   "id": "e077ab61e95db3b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:43:15.523941Z",
     "start_time": "2025-01-14T12:42:55.838551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scroll_page(driver, pause_time=1.5):\n",
    "    \"\"\"Scroll the page and return True if new content was loaded\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(pause_time)  # Wait for content to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    return new_height > last_height\n",
    "\n",
    "def scrape_techinasia_ai_news(num_articles=50, max_scrolls=10):\n",
    "    articles = []\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "\n",
    "    try:\n",
    "        print(\"Starting Chrome webdriver...\")\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        url = 'https://www.techinasia.com/news?category=artificial-intelligence'\n",
    "        print(f\"Fetching URL: {url}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        print(\"Waiting for initial content to load...\")\n",
    "\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'post-card')))\n",
    "\n",
    "        # Scroll to load more articles\n",
    "        scroll_count = 0\n",
    "        while len(articles) < num_articles and scroll_count < max_scrolls:\n",
    "            if scroll_page(driver):\n",
    "                scroll_count += 1\n",
    "                print(f\"Scrolled page {scroll_count} times, loading more articles...\")\n",
    "            else:\n",
    "                print(\"No more new content loaded\")\n",
    "                break\n",
    "\n",
    "        print(\"Parsing content...\")\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        article_elements = soup.find_all('article', class_='post-card')\n",
    "        print(f\"Found {len(article_elements)} articles\")\n",
    "\n",
    "        for i, article in enumerate(article_elements, 1):\n",
    "            try:\n",
    "                content_div = article.find('div', class_='post-content')\n",
    "\n",
    "                # Get title and its link\n",
    "                title_element = content_div.find('h3', class_='post-title')\n",
    "                title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "                # Get article URL and extract article ID\n",
    "                article_links = [a for a in content_div.find_all('a') if not 'post-source' in a.get('class', [])]\n",
    "                article_url = None\n",
    "                article_id = None\n",
    "                if article_links:\n",
    "                    href = article_links[0]['href']\n",
    "                    article_url = f\"https://www.techinasia.com{href}\" if not href.startswith('http') else href\n",
    "                    # Extract article ID from URL\n",
    "                    article_id = href.split('/')[-1] if href else None\n",
    "\n",
    "                # Get source information\n",
    "                source_element = content_div.find('span', class_='post-source-name')\n",
    "                source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "                source_link = content_div.find('a', class_='post-source')\n",
    "                source_url = source_link['href'] if source_link else None\n",
    "\n",
    "                # Get image information\n",
    "                image_div = article.find('div', class_='post-image')\n",
    "                image_url = None\n",
    "                if image_div:\n",
    "                    img_tag = image_div.find('img')\n",
    "                    if img_tag:\n",
    "                        image_url = img_tag.get('src')\n",
    "\n",
    "                # Get time and tags\n",
    "                footer = article.find('div', class_='post-footer')\n",
    "                time_element = footer.find('time') if footer else None\n",
    "                posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "                relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "                # Get categories and tags\n",
    "                categories = []\n",
    "                tags = []\n",
    "                if footer:\n",
    "                    tag_elements = footer.find_all('a', class_='post-taxonomy-link')\n",
    "                    for tag in tag_elements:\n",
    "                        tag_text = tag.text.strip('· ')\n",
    "                        if tag_text:\n",
    "                            if tag.get('href', '').startswith('/category/'):\n",
    "                                categories.append(tag_text)\n",
    "                            elif tag.get('href', '').startswith('/tag/'):\n",
    "                                tags.append(tag_text)\n",
    "\n",
    "                article_data = {\n",
    "                    'article_id': article_id,\n",
    "                    'title': title,\n",
    "                    'article_url': article_url,\n",
    "                    'source': source,\n",
    "                    'source_url': source_url,\n",
    "                    'image_url': image_url,\n",
    "                    'posted_time': posted_time,\n",
    "                    'relative_time': relative_time,\n",
    "                    'categories': ', '.join(categories) if categories else 'N/A',\n",
    "                    'tags': ', '.join(tags) if tags else 'N/A',\n",
    "                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "\n",
    "                print(f\"Processing article {i}: {title[:50]}...\")\n",
    "                articles.append(article_data)\n",
    "\n",
    "                if len(articles) >= num_articles:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article {i}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"Closing browser...\")\n",
    "        driver.quit()\n",
    "\n",
    "    if not articles:\n",
    "        print(\"No articles were collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    print(f\"\\nSuccessfully collected {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def save_results(df):\n",
    "    if df.empty:\n",
    "        print(\"No data to save\")\n",
    "        return None\n",
    "\n",
    "    filename = f'techinasia_ai_news_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nSaved {len(df)} articles to {filename}\")\n",
    "\n",
    "    print(\"\\nSample of collected data:\")\n",
    "    display_columns = ['title', 'source', 'posted_time', 'categories', 'tags']\n",
    "    print(df[display_columns].head())\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Run the scraper with increased article count\n",
    "print(\"Starting enhanced scraper...\")\n",
    "df = scrape_techinasia_ai_news(num_articles=50, max_scrolls=10)\n",
    "save_results(df)"
   ],
   "id": "adfec16f15f94241",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced scraper...\n",
      "Starting Chrome webdriver...\n",
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "Waiting for initial content to load...\n",
      "Scrolled page 1 times, loading more articles...\n",
      "Scrolled page 2 times, loading more articles...\n",
      "Scrolled page 3 times, loading more articles...\n",
      "Scrolled page 4 times, loading more articles...\n",
      "Scrolled page 5 times, loading more articles...\n",
      "Scrolled page 6 times, loading more articles...\n",
      "Scrolled page 7 times, loading more articles...\n",
      "Scrolled page 8 times, loading more articles...\n",
      "Scrolled page 9 times, loading more articles...\n",
      "Scrolled page 10 times, loading more articles...\n",
      "Parsing content...\n",
      "Found 165 articles\n",
      "Processing article 1: AWS, General Catalyst advance AI tools to support ...\n",
      "Processing article 2: Trump likely to keep US AI chip export restriction...\n",
      "Processing article 3: OpenAI urges action to maintain US AI leadership...\n",
      "Processing article 4: TSMC Q4 profit set to surge 58% on strong AI chip ...\n",
      "Processing article 5: PUBG maker Krafton to invest $136m in game studios...\n",
      "Processing article 6: Microsoft forms new engineering group to boost AI ...\n",
      "Processing article 7: US tighten AI chip exports, targets China...\n",
      "Processing article 8: Indonesian AI character generator nets $5m seed...\n",
      "Processing article 9: Google’s AI agent enhances voice assist in Mercede...\n",
      "Processing article 10: Adobe launches new AI tool for bulk image editing...\n",
      "Processing article 11: Chinese AI startup DeepSeek launches free iOS app...\n",
      "Processing article 12: South Korea boosts AI, semiconductor ties with Isr...\n",
      "Processing article 13: SK Telecom launches GPUaaS for South Korea’s AI da...\n",
      "Processing article 14: YouTubers sell unused video footage to AI firms...\n",
      "Processing article 15: Singtel launches Perplexity Pro AI with free year ...\n",
      "Processing article 16: China’s DeepSeek relies on young team to challenge...\n",
      "Processing article 17: Chinese firms showcase AI robots, EVs at CES 2025...\n",
      "Processing article 18: Alibaba Cloud launches AI tool for faster app deve...\n",
      "Processing article 19: Hyundai partners with Nvidia to advance AI in mobi...\n",
      "Processing article 20: Ant Group to connect 280k doctors on Haodf healthc...\n",
      "Processing article 21: OpenAI, Meta, Uber execs set to attend Trump’s ina...\n",
      "Processing article 22: OpenAI considers high-volume production of humanoi...\n",
      "Processing article 23: US agencies partially back Musk’s legal challenge ...\n",
      "Processing article 24: Microsoft sues group over Azure OpenAI service...\n",
      "Processing article 25: Nvidia raises concerns over Biden’s reported expor...\n",
      "Processing article 26: TSMC achieves $26.3b in Q4 revenue due to growing ...\n",
      "Processing article 27: Musk’s attorney urges OpenAI auction: sources...\n",
      "Processing article 28: Perplexity integrates Tripadvisor to improve hotel...\n",
      "Processing article 29: Meta faces allegations of using pirated books for ...\n",
      "Processing article 30: Healthcare startup Hippocratic AI raises $141m in ...\n",
      "Processing article 31: Nvidia, Netherlands team up on $210m AI research f...\n",
      "Processing article 32: Inshorts co-founder launches AI platform for non-c...\n",
      "Processing article 33: SK Telecom, SK hynix, Penguin partner on AI data c...\n",
      "Processing article 34: Delta Air Lines unveils AI assistant for travel, b...\n",
      "Processing article 35: X Corp launches Grok chatbot app in beta for iOS u...\n",
      "Processing article 36: HK construction tech company raises $3m series A...\n",
      "Processing article 37: AI boosts bank profits but costs 200k jobs worldwi...\n",
      "Processing article 38: Nvidia unveils breakthrough AI chips at CES 2025...\n",
      "Processing article 39: Chip export controls threaten US AI leadership: in...\n",
      "Processing article 40: AI jobs top LinkedIn’s US job growth list...\n",
      "Processing article 41: Indian deeptech firm names CTO Ayush Gupta as co-f...\n",
      "Processing article 42: Microsoft invests $3b in India with public, privat...\n",
      "Processing article 43: Tesla explosion outside Trump Hotel linked to genA...\n",
      "Processing article 44: CES 2025 showcases AI, self-driving cars...\n",
      "Processing article 45: South Koreans spend 900m minutes on AI services...\n",
      "Processing article 46: Apple to update AI summaries after misinformation ...\n",
      "Processing article 47: Half of Saudi deeptech startups focus on AI, IoT: ...\n",
      "Processing article 48: Google DeepMind forms team to improve AI, world mo...\n",
      "Processing article 49: China’s 01.AI founder denies selling assets to Ali...\n",
      "Processing article 50: Chinese healthcare firm Weimai raises $27.3m serie...\n",
      "Closing browser...\n",
      "\n",
      "Successfully collected 50 articles\n",
      "\n",
      "Saved 50 articles to techinasia_ai_news_20250114_204315.csv\n",
      "\n",
      "Sample of collected data:\n",
      "                                               title     source  \\\n",
      "0  AWS, General Catalyst advance AI tools to supp...       CNBC   \n",
      "1  Trump likely to keep US AI chip export restric...  Bloomberg   \n",
      "2   OpenAI urges action to maintain US AI leadership    Reuters   \n",
      "3  TSMC Q4 profit set to surge 58% on strong AI c...    Reuters   \n",
      "4  PUBG maker Krafton to invest $136m in game stu...  Bloomberg   \n",
      "\n",
      "               posted_time       categories   tags  \n",
      "0  6:05 PM at Jan 14, 2025               AI    N/A  \n",
      "1  5:56 PM at Jan 14, 2025               AI  China  \n",
      "2  3:38 PM at Jan 14, 2025               AI  China  \n",
      "3  3:15 PM at Jan 14, 2025               AI    N/A  \n",
      "4  1:02 PM at Jan 14, 2025  AI, Investments    N/A  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'techinasia_ai_news_20250114_204315.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64a53fdf6a846cae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For fewer articles:\n",
    "df = scrape_techinasia_ai_news(num_articles=20, max_scrolls=5)\n",
    "\n",
    "# For more articles:\n",
    "df = scrape_techinasia_ai_news(num_articles=100, max_scrolls=20)"
   ],
   "id": "c76c5ad354688de7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:46:40.473269Z",
     "start_time": "2025-01-14T12:46:27.493938Z"
    }
   },
   "cell_type": "code",
   "source": "df = scrape_techinasia_ai_news(num_articles=20, max_scrolls=5)",
   "id": "a85088c61fe19edc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Chrome webdriver...\n",
      "Fetching URL: https://www.techinasia.com/news?category=artificial-intelligence\n",
      "Waiting for initial content to load...\n",
      "Scrolled page 1 times, loading more articles...\n",
      "Scrolled page 2 times, loading more articles...\n",
      "Scrolled page 3 times, loading more articles...\n",
      "Scrolled page 4 times, loading more articles...\n",
      "Scrolled page 5 times, loading more articles...\n",
      "Parsing content...\n",
      "Found 90 articles\n",
      "Processing article 1: AWS, General Catalyst advance AI tools to support ...\n",
      "Processing article 2: Trump likely to keep US AI chip export restriction...\n",
      "Processing article 3: OpenAI urges action to maintain US AI leadership...\n",
      "Processing article 4: TSMC Q4 profit set to surge 58% on strong AI chip ...\n",
      "Processing article 5: PUBG maker Krafton to invest $136m in game studios...\n",
      "Processing article 6: Microsoft forms new engineering group to boost AI ...\n",
      "Processing article 7: US tighten AI chip exports, targets China...\n",
      "Processing article 8: Indonesian AI character generator nets $5m seed...\n",
      "Processing article 9: Google’s AI agent enhances voice assist in Mercede...\n",
      "Processing article 10: Adobe launches new AI tool for bulk image editing...\n",
      "Processing article 11: Chinese AI startup DeepSeek launches free iOS app...\n",
      "Processing article 12: South Korea boosts AI, semiconductor ties with Isr...\n",
      "Processing article 13: SK Telecom launches GPUaaS for South Korea’s AI da...\n",
      "Processing article 14: YouTubers sell unused video footage to AI firms...\n",
      "Processing article 15: Singtel launches Perplexity Pro AI with free year ...\n",
      "Processing article 16: China’s DeepSeek relies on young team to challenge...\n",
      "Processing article 17: Chinese firms showcase AI robots, EVs at CES 2025...\n",
      "Processing article 18: Alibaba Cloud launches AI tool for faster app deve...\n",
      "Processing article 19: Hyundai partners with Nvidia to advance AI in mobi...\n",
      "Processing article 20: Ant Group to connect 280k doctors on Haodf healthc...\n",
      "Closing browser...\n",
      "\n",
      "Successfully collected 20 articles\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "v",
   "id": "92ff7067435895cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## v1",
   "id": "fbad4789e8d59af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "506f86b1f358b5c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
