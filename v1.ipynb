{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T12:52:12.457544Z",
     "start_time": "2025-01-14T12:52:00.856253Z"
    }
   },
   "source": [
    "# techinasia_scraper_v1.py\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from random import uniform\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper_v1.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Article:\n",
    "    \"\"\"Data class for storing article information\"\"\"\n",
    "    article_id: str\n",
    "    title: str\n",
    "    article_url: str\n",
    "    source: str\n",
    "    source_url: str\n",
    "    image_url: Optional[str]\n",
    "    posted_time: str\n",
    "    relative_time: str\n",
    "    categories: List[str]\n",
    "    tags: List[str]\n",
    "    scraped_at: str\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'article_id': self.article_id,\n",
    "            'title': self.title,\n",
    "            'article_url': self.article_url,\n",
    "            'source': self.source,\n",
    "            'source_url': self.source_url,\n",
    "            'image_url': self.image_url,\n",
    "            'posted_time': self.posted_time,\n",
    "            'posted_time_iso': self.get_iso_time(),\n",
    "            'relative_time': self.relative_time,\n",
    "            'categories': ','.join(self.categories),\n",
    "            'tags': ','.join(self.tags),\n",
    "            'scraped_at': self.scraped_at\n",
    "        }\n",
    "\n",
    "    def get_iso_time(self) -> Optional[str]:\n",
    "        \"\"\"Convert posted_time to ISO format\"\"\"\n",
    "        try:\n",
    "            dt = datetime.strptime(self.posted_time, '%I:%M %p at %b %d, %Y')\n",
    "            return dt.isoformat()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "class ScraperConfig:\n",
    "    \"\"\"Configuration management for the scraper\"\"\"\n",
    "    DEFAULT_CONFIG = {\n",
    "        'num_articles': 50,\n",
    "        'max_scrolls': 10,\n",
    "        'timeout': 20,\n",
    "        'retry_count': 3,\n",
    "        'scroll_pause_time': 1.5,\n",
    "        'batch_size': 100,\n",
    "        'base_url': 'https://www.techinasia.com/news',\n",
    "        'output_dir': 'output'\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.config = {**self.DEFAULT_CONFIG, **kwargs}\n",
    "        self.__dict__.update(self.config)\n",
    "\n",
    "class TechInAsiaScraper:\n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"Initialize the scraper with configuration\"\"\"\n",
    "        self.config = ScraperConfig(**(config or {}))\n",
    "        self.driver = None\n",
    "        self._setup_output_directory()\n",
    "\n",
    "    def _setup_output_directory(self):\n",
    "        \"\"\"Create output directory if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(self.config.output_dir):\n",
    "            os.makedirs(self.config.output_dir)\n",
    "\n",
    "    def setup_driver(self):\n",
    "        \"\"\"Initialize Selenium WebDriver with retry mechanism\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "\n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(\n",
    "                service=Service(ChromeDriverManager().install()),\n",
    "                options=options\n",
    "            )\n",
    "            logger.info(\"WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize WebDriver: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scroll_page(self) -> bool:\n",
    "        \"\"\"Scroll the page and return True if new content was loaded\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(uniform(1.0, self.config.scroll_pause_time))\n",
    "        new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        return new_height > last_height\n",
    "\n",
    "    def parse_article(self, article_element) -> Optional[Article]:\n",
    "        \"\"\"Parse a single article element\"\"\"\n",
    "        try:\n",
    "            content_div = article_element.find('div', class_='post-content')\n",
    "\n",
    "            # Extract article information\n",
    "            title_element = content_div.find('h3', class_='post-title')\n",
    "            title = title_element.text.strip() if title_element else 'N/A'\n",
    "\n",
    "            # Get article URL and ID\n",
    "            article_links = [a for a in content_div.find_all('a') if not 'post-source' in a.get('class', [])]\n",
    "            article_url = None\n",
    "            article_id = None\n",
    "            if article_links:\n",
    "                href = article_links[0]['href']\n",
    "                article_url = f\"https://www.techinasia.com{href}\" if not href.startswith('http') else href\n",
    "                article_id = href.split('/')[-1] if href else None\n",
    "\n",
    "            # Get source information\n",
    "            source_element = content_div.find('span', class_='post-source-name')\n",
    "            source = source_element.text.strip() if source_element else 'N/A'\n",
    "\n",
    "            source_link = content_div.find('a', class_='post-source')\n",
    "            source_url = source_link['href'] if source_link else None\n",
    "\n",
    "            # Get image information\n",
    "            image_div = article_element.find('div', class_='post-image')\n",
    "            image_url = None\n",
    "            if image_div:\n",
    "                img_tag = image_div.find('img')\n",
    "                if img_tag:\n",
    "                    image_url = img_tag.get('src')\n",
    "\n",
    "            # Get time and categories/tags\n",
    "            footer = article_element.find('div', class_='post-footer')\n",
    "            time_element = footer.find('time') if footer else None\n",
    "            posted_time = time_element['datetime'] if time_element else 'N/A'\n",
    "            relative_time = time_element.text.strip() if time_element else 'N/A'\n",
    "\n",
    "            # Parse categories and tags\n",
    "            categories = []\n",
    "            tags = []\n",
    "            if footer:\n",
    "                tag_elements = footer.find_all('a', class_='post-taxonomy-link')\n",
    "                for tag in tag_elements:\n",
    "                    tag_text = tag.text.strip('Â· ')\n",
    "                    if tag_text:\n",
    "                        if tag.get('href', '').startswith('/category/'):\n",
    "                            categories.append(tag_text)\n",
    "                        elif tag.get('href', '').startswith('/tag/'):\n",
    "                            tags.append(tag_text)\n",
    "\n",
    "            return Article(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                article_url=article_url,\n",
    "                source=source,\n",
    "                source_url=source_url,\n",
    "                image_url=image_url,\n",
    "                posted_time=posted_time,\n",
    "                relative_time=relative_time,\n",
    "                categories=categories,\n",
    "                tags=tags,\n",
    "                scraped_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing article: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape(self) -> pd.DataFrame:\n",
    "        \"\"\"Main scraping method\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            self.setup_driver()\n",
    "            articles = self._scrape_with_retry()\n",
    "            return self._process_articles(articles)\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "\n",
    "    def _scrape_with_retry(self) -> List[Article]:\n",
    "        \"\"\"Implement retry logic for scraping\"\"\"\n",
    "        for attempt in range(self.config.retry_count):\n",
    "            try:\n",
    "                return self._perform_scraping()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == self.config.retry_count - 1:\n",
    "                    raise\n",
    "                time.sleep(2 * (attempt + 1))  # Exponential backoff\n",
    "\n",
    "    def _perform_scraping(self) -> List[Article]:\n",
    "        \"\"\"Perform the actual scraping\"\"\"\n",
    "        articles = []\n",
    "        url = f\"{self.config.base_url}?category=artificial-intelligence\"\n",
    "\n",
    "        self.driver.get(url)\n",
    "        wait = WebDriverWait(self.driver, self.config.timeout)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'post-card')))\n",
    "\n",
    "        scroll_count = 0\n",
    "        while len(articles) < self.config.num_articles and scroll_count < self.config.max_scrolls:\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            article_elements = soup.find_all('article', class_='post-card')\n",
    "\n",
    "            for article_element in article_elements:\n",
    "                if len(articles) >= self.config.num_articles:\n",
    "                    break\n",
    "\n",
    "                article = self.parse_article(article_element)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "\n",
    "            if not self.scroll_page():\n",
    "                break\n",
    "            scroll_count += 1\n",
    "            logger.info(f\"Scrolled {scroll_count} times, found {len(articles)} articles\")\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def _process_articles(self, articles: List[Article]) -> pd.DataFrame:\n",
    "        \"\"\"Process and clean scraped articles\"\"\"\n",
    "        if not articles:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Convert articles to DataFrame\n",
    "        df = pd.DataFrame([article.to_dict() for article in articles])\n",
    "\n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['article_url'], keep='first')\n",
    "\n",
    "        # Save in batches\n",
    "        self._save_batches(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _save_batches(self, df: pd.DataFrame):\n",
    "        \"\"\"Save DataFrame in batches\"\"\"\n",
    "        batch_size = self.config.batch_size\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df[i:i + batch_size]\n",
    "            filename = f\"{self.config.output_dir}/techinasia_ai_news_v1_batch_{i//batch_size}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            batch.to_csv(filename, index=False)\n",
    "            logger.info(f\"Saved batch {i//batch_size + 1} to {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "    config = {\n",
    "        'num_articles': 100,\n",
    "        'max_scrolls': 15,\n",
    "        'output_dir': 'techinasia_output'\n",
    "    }\n",
    "\n",
    "    logger.info(\"Starting TechInAsia scraper v1\")\n",
    "    scraper = TechInAsiaScraper(config)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape()\n",
    "        logger.info(f\"Successfully scraped {len(df)} articles\")\n",
    "\n",
    "        # Display sample of results\n",
    "        if not df.empty:\n",
    "            print(\"\\nSample of scraped articles:\")\n",
    "            display_columns = ['title', 'source', 'posted_time_iso', 'categories', 'tags']\n",
    "            print(df[display_columns].head())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 20:52:01,586 - INFO - Starting TechInAsia scraper v1\n",
      "2025-01-14 20:52:01,588 - INFO - ====== WebDriver manager ======\n",
      "2025-01-14 20:52:02,083 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-01-14 20:52:02,196 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-01-14 20:52:02,286 - INFO - Driver [/Users/apple/.wdm/drivers/chromedriver/mac64/131.0.6778.264/chromedriver-mac-x64/chromedriver] found in cache\n",
      "2025-01-14 20:52:03,240 - INFO - WebDriver initialized successfully\n",
      "2025-01-14 20:52:07,912 - INFO - Scrolled 1 times, found 15 articles\n",
      "2025-01-14 20:52:09,500 - INFO - Scrolled 2 times, found 45 articles\n",
      "2025-01-14 20:52:10,995 - INFO - Scrolled 3 times, found 90 articles\n",
      "2025-01-14 20:52:12,308 - INFO - Scrolled 4 times, found 100 articles\n",
      "2025-01-14 20:52:12,318 - INFO - Saved batch 1 to techinasia_output/techinasia_ai_news_v1_batch_0_20250114_205212.csv\n",
      "2025-01-14 20:52:12,449 - INFO - Successfully scraped 45 articles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of scraped articles:\n",
      "                                               title     source  \\\n",
      "0  AWS, General Catalyst advance AI tools to supp...       CNBC   \n",
      "1  Trump likely to keep US AI chip export restric...  Bloomberg   \n",
      "2   OpenAI urges action to maintain US AI leadership    Reuters   \n",
      "3  TSMC Q4 profit set to surge 58% on strong AI c...    Reuters   \n",
      "4  PUBG maker Krafton to invest $136m in game stu...  Bloomberg   \n",
      "\n",
      "       posted_time_iso      categories   tags  \n",
      "0  2025-01-14T18:05:00              AI         \n",
      "1  2025-01-14T17:56:00              AI  China  \n",
      "2  2025-01-14T15:38:00              AI  China  \n",
      "3  2025-01-14T15:15:00              AI         \n",
      "4  2025-01-14T13:02:00  AI,Investments         \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3663abcf2d8d5863"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
